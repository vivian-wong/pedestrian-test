{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install detectron2 if want to recognize different types of pedestrians\n",
    "# installation instructions for M1 Mac: https://medium.com/@hakon.hukkelas/installing-detectron2-natively-for-mac-m1-pro-apple-silicon-a89517f1c913\n",
    "#!pip install git+https://github.com/facebookresearch/detectron2@main\n",
    "# add cfg.MODEL.DEVICE = 'cpu' to your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manimation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FuncAnimation\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# from celluloid import Camera\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Use SORT to track people\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msort\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# # setup detectron2\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# import detectron2\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# from detectron2.utils.logger import setup_logger\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# from detectron2.utils.visualizer import ColorMode, Visualizer\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# from detectron2.data import MetadataCatalog, DatasetCatalog\u001b[39;00m\n\u001b[1;32m     39\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sort'"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import os, sys, time, datetime, random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import utm\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from celluloid import Camera\n",
    "\n",
    "\n",
    "# Use SORT to track people\n",
    "from sort import *\n",
    "\n",
    "# setup detectron2\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.video_visualizer import VideoVisualizer\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(\"torch version\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be changed \n",
    "video_filename = 'Y2E2_West.MOV' # put video in './data' directory\n",
    "detector = 'detectron2' # yolov3, faster_rcnn, detectron2\n",
    "\n",
    "# precomputed homography matrices\n",
    "############# STADIUM #############\n",
    "# pts_img = [[450.0,203.0],[282.0,297.0],[470.0,403.0],[630.0,270.0]] # top left, bot left, bot right, top right \n",
    "# pts_world = [[250,250], [250,500], [500,500],[500,250]]\n",
    "\n",
    "# HOMOG, status = cv2.findHomography(np.array(pts_img), np.array(pts_world)+700)\n",
    "\n",
    "############# GCS #############\n",
    "# HOMOG = [[4.97412897e-02, -4.24730883e-02, 7.25543911e+01],\n",
    "#          [1.45017874e-01, -3.35678711e-03, 7.97920970e+00],\n",
    "#          [1.36068797e-03, -4.98339188e-05, 1.00000000e+00]] \n",
    "\n",
    "############# Y2E2 West #############\n",
    "#note: points must be consistent in order and winding (e.g. clockwise) \n",
    "pts_img = [[1090, 1625], [2270, 1785], [2010, 2250], [470, 2150]] # palm tree locations \n",
    "long_lat_world = [(37.428570, -122.175184), (37.428489, -122.175211), \n",
    "                  (37.428506, -122.175288), (37.428589, -122.175260)]\n",
    "pts_world = []\n",
    "for lat,lon in long_lat_world: \n",
    "    utm_east, utm_north, utm_zone, utm_letter = utm.from_latlon(lat,lon) # meters\n",
    "    pts_world.append([utm_east*100, utm_north*100]) # centimeters\n",
    "# shift pts_world so that values aren't too big\n",
    "world_np = np.array(pts_world)\n",
    "world_np = world_np - world_np.min(axis=0)\n",
    "pts_world = world_np.tolist()\n",
    "\n",
    "    \n",
    "HOMOG, status = cv2.findHomography(np.array(pts_img), np.array(pts_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually don't need to change\n",
    "assumed_height = 174 # cm \n",
    "history_size = 5\n",
    "speed_tolerance = 20\n",
    "\n",
    "yolov3_weight_path = 'weights/yolov3.weights'\n",
    "yolov3_config_path='config/yolov3.cfg'\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%m%d%y_%H%M%S\")\n",
    "path_result = os.path.join('results', detector, video_filename.split('.')[0], current_time)\n",
    "os.makedirs(path_result, exist_ok=True)\n",
    "result_path_detection = os.path.join(path_result, video_filename)\n",
    "result_path_world = result_path_detection.replace('.', '_2D.',1)\n",
    "\n",
    "scale_size = None\n",
    "conf_thres=None\n",
    "nms_thres=None\n",
    "\n",
    "if detector=='yolov3':\n",
    "    scale_size = 416\n",
    "    conf_thres=0.8\n",
    "    nms_thres=0.4\n",
    "    CLASSES = utils.load_classes('config/coco.names') # 80 classes for darknet pretraiend\n",
    "elif detector=='faster_rcnn': \n",
    "    # 91 classes torchvision pretrained\n",
    "    # from https://pytorch.org/vision/stable/models.html#id57\n",
    "    CLASSES = [ \n",
    "        '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "        'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "        'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "        'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "        'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "        'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "        'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "        'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "        'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "    ]\n",
    "elif detector=='detectron2': \n",
    "    # segmentation\n",
    "    CLASSES = utils.load_classes('config/coco.names') # 80 classes for detectron2 pretraiend\n",
    "    cfg = get_cfg()\n",
    "    if not torch.cuda.is_available():\n",
    "        cfg.MODEL.DEVICE = 'cpu' \n",
    "    \n",
    "    #### Coco segmentation ####\n",
    "    # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    \n",
    "    #### Coco keypoint ####\n",
    "#     cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "#     cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "#     cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "    \n",
    "    #### Coco panoptic segmentation #### TODO\n",
    "#     cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
    "#     cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "#     cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
    "else: \n",
    "    raise Exception('Invalid detector name')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Only - Panoptic Segmentation w/ Detectron2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### converting any coordinate to world coordinate \n",
    "def img_to_world_coords(coords_img, HOMOG, pts_img = None, pts_world = None):\n",
    "    '''\n",
    "    use at least 4 \n",
    "    input: \n",
    "        coords_img: a np array of dim (num_ped,seq_len,2)\n",
    "    if using pre-computed HOMOG, no need to use pts_img and pts_world\n",
    "        pts_img: a list of [x,y] points in image coordinates. list length cannot be shorter than 4\n",
    "        pts_world: a list of [x,y] points in world coordinates. each point cooresponds to a point in pts_img\n",
    "        assumed_height: the estimated height of all humans. source: https://www.sciencedirect.com/science/article/pii/S0379073811002167\n",
    "                        NOT USED FOR NOW \n",
    "    output: \n",
    "        coords_world: same shape as coords_img but in world coordinates\n",
    "    '''\n",
    "    # convert to np matrix and check size\n",
    "    if pts_img == None: \n",
    "        h = HOMOG\n",
    "    else: \n",
    "        pts_img = np.array(pts_img)\n",
    "        pts_world = np.array(pts_world)\n",
    "        assert pts_img.shape==pts_world.shape and pts_img.shape[0] >= 4\n",
    "     \n",
    "        # calculate homography matrix H. more pts_img means more accurate H\n",
    "        h, status = cv2.findHomography(pts_img, pts_world)\n",
    "     \n",
    "    # finally, get the mapped world coordinates\n",
    "    coords_world = cv2.perspectiveTransform(coords_img.astype(np.float32), h)\n",
    "    # coords_world[:,1] += assumed_height\n",
    "    return coords_world\n",
    "\n",
    "def calc_num_violated (all_pedestrians):\n",
    "    num_violated = 0\n",
    "    \n",
    "#     for p in all_pedestrians: \n",
    "#         min_y = 100000\n",
    "#         if p.y2 < min_y: \n",
    "#             min_y = p.y2\n",
    "#             closest_p = p\n",
    "\n",
    "#     radius_cm = closest_p.box_w * assumed_height / closest_p.box_h / 2\n",
    "#     worlddist2cm = radius_cm / closest_p.radius \n",
    "#     print(worlddist2cm)\n",
    "    from itertools import combinations \n",
    "\n",
    "    for (p1,p2) in combinations(all_pedestrians, 2): \n",
    "        if p1.is_violating_sd(p2):\n",
    "            assert p2.is_violating_sd(p1)\n",
    "            p1.is_violating=True\n",
    "            p2.is_violating=True\n",
    "    \n",
    "    for p in all_pedestrians: \n",
    "        if p.is_violating:\n",
    "            num_violated += 1\n",
    "            \n",
    "    return num_violated\n",
    "\n",
    "def draw_circle(frame, center, radius, color=None): \n",
    "    cv2.circle(frame, (int(center[0]), int(center[1])), int(radius), color=color, thickness=10)\n",
    "\n",
    "def draw_point(frame, center): \n",
    "    draw_circle(frame, center, 1, color=(0,255,0))\n",
    "\n",
    "def detect_image(frame, model, detector_name, scale_size = None):\n",
    "    '''\n",
    "    input: a SINGLE opencv (BGR) image, usually just a frame from cv2.videocapture.read()\n",
    "    returns: \n",
    "    predictions - a prediction dictionary, containing output in 2 formats:\n",
    "                      prediction[\"darknet\"] = FloatTensor[N,6], each row is x1, y1, x2, y2, class_score, class_pred\n",
    "                      prediction[\"detectron2\"] = original detectron2 output.\n",
    "                      prediction[\"torchvision\"] = torchvision format. \n",
    "    '''\n",
    "    prediction = dict()\n",
    "    \n",
    "    # make sure model is correct mode\n",
    "    if detector is not 'detectron2':\n",
    "        model.to(device=device)\n",
    "        model.eval()\n",
    "    \n",
    "    # scaling and padding (optional)\n",
    "    rgb_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    if scale_size: \n",
    "        pilimg = Image.fromarray(rgb_img) # pillow image \n",
    "        img_size = scale_size\n",
    "        ratio = min(img_size/pilimg.size[0], img_size/pilimg.size[1])\n",
    "        imw = round(pilimg.size[0] * ratio)\n",
    "        imh = round(pilimg.size[1] * ratio)\n",
    "        img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
    "             transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
    "                            (128,128,128)),\n",
    "             transforms.ToTensor(),\n",
    "             ])\n",
    "        # convert image to Tensor\n",
    "        rgb_tensor = img_transforms(pilimg).float()\n",
    "        \n",
    "    else: \n",
    "        # convert rgb image to Tensor\n",
    "        rgb_tensor = np.moveaxis(rgb_img, -1, 0) / 255 # convert to (C,H,W) and [0-1]\n",
    "        rgb_tensor = torch.tensor(rgb_tensor, device=device).float()\n",
    "    \n",
    "    ######## torchvision model detection ########\n",
    "    '''\n",
    "    Getting predictions, torchvision style (https://pytorch.org/vision/stable/models.html#id57)\n",
    "    - input: list of tensors, one for each image. Tensor shape [C,H,W], values within [0,1]\n",
    "    - return: List[Dict[Tensor]], one dict for each image\n",
    "        \"boxes\": FloatTensor[N,4] in [x1,y1,x2,y2] format\n",
    "        \"labels\": Int64Tensor[N] \n",
    "        \"scores\": Tensor[N]\n",
    "    - We here convert predictions to be consistent with darknet format:\n",
    "        FloatTensor[N,7]\n",
    "        each row is x1, y1, x2, y2, class_score, class_pred\n",
    "    '''\n",
    "    if detector_name == 'faster_rcnn':\n",
    "        # torchvision wants RGB input\n",
    "        with torch.no_grad():\n",
    "            prediction[\"torchvision\"] = model([rgb_tensor]) # takes input as a list of tensor\n",
    "        for pred_dict in prediction[\"torchvision\"]: \n",
    "            boxes = pred_dict['boxes'] # (N,4)\n",
    "            classIDs = pred_dict['labels'] # (N)\n",
    "            scores = pred_dict['scores'] # (N)    \n",
    "            \n",
    "            num_boxes = boxes.shape[0]\n",
    "            prediction[\"darknet\"] = torch.cat((boxes, scores.unsqueeze(1), classIDs.unsqueeze(1)),dim=1)\n",
    "    \n",
    "    ######## YOLOV3 model detection ########\n",
    "    elif detector_name == 'yolov3': \n",
    "        '''\n",
    "        - Input: [num_image,C,H,W]\n",
    "        - Darknet returns: [centerx, centery, width, height, score, classA prob, ... classB prob]\n",
    "          utils.non_max_suppresion converts results to list of num_image tensors:FloatTensor[N,7]\n",
    "              each row is x1, y1, x2, y2, object_conf, class_score, class_pred\n",
    "        '''\n",
    "        # darknet wants RGB input\n",
    "        with torch.no_grad():\n",
    "            rgb_tensor = torch.unsqueeze(rgb_tensor, dim=0)\n",
    "            predictions = model(rgb_tensor) \n",
    "            print(predictions)\n",
    "        predictions = utils.non_max_suppression(predictions, num_classes=len(CLASSES), conf_thres=conf_thres, nms_thres=nms_thres)\n",
    "        prediction[\"darknet\"] = predictions[0] # [0] because 1 image only. \n",
    "        # remove column containing object_conf. \n",
    "        prediction[\"darknet\"] = torch.cat([prediction[\"darknet\"], prediction[\"darknet\"]],dim=1)\n",
    "        \n",
    "    ######## detectron2 panoptic segmentation ########\n",
    "    elif detector_name == 'detectron2': \n",
    "        '''\n",
    "        - Input: cv2 image \n",
    "        - Return: \n",
    "        '''\n",
    "        # detectron2 wants opencv BGR input, so dont need to use RGB image. \n",
    "        prediction[\"detectron2\"] = model(frame) # output format: https://detectron2.readthedocs.io/en/latest/tutorials/models.html#model-output-format\n",
    "        # convert to darknet format\n",
    "        N = len(prediction[\"detectron2\"][\"instances\"].pred_boxes)\n",
    "        prediction[\"darknet\"] = torch.FloatTensor(N, 6).to(device) \n",
    "        prediction[\"darknet\"][:,0:4] = prediction[\"detectron2\"][\"instances\"].pred_boxes.tensor # first 4 columns: x1, y1, x2, y2\n",
    "        prediction[\"darknet\"][:,4] = prediction[\"detectron2\"][\"instances\"].scores\n",
    "        prediction[\"darknet\"][:,5] = prediction[\"detectron2\"][\"instances\"].pred_classes\n",
    "    else: \n",
    "        raise Exception('Invalid detector name')\n",
    "    return prediction\n",
    "\n",
    "##### custom pedestrian class #####\n",
    "class Pedestrian:\n",
    "    def __init__(self, coords1, coords2, pedestrian_id, cls, box_h, box_w):\n",
    "        self.x1, self.y1 = coords1 # top lef\n",
    "        self.x2, self.y2 = coords2 # bot right \n",
    "        self.id = pedestrian_id # in this frame \n",
    "        self.cls = cls # either 'person' or 'bicycle'\n",
    "        self.box_h = box_h\n",
    "        self.box_w = box_w\n",
    "\n",
    "        feet = img_to_world_coords(np.array([[[self.x1, self.y2],[self.x2,self.y2]]]), HOMOG).squeeze()\n",
    "        feetL = feet[0,:]\n",
    "        feetR = feet[1,:]\n",
    "        \n",
    "        self.radius = np.linalg.norm(feetL - feetR)/2 \n",
    "        \n",
    "#         center_x =(feetL[0] + feetR[0])/2\n",
    "#         center_y = (feetL[1] + feetR[1])/2\n",
    "        self.center = (feetL + feetR)/2 # np.array([center_x, center_y])\n",
    "        self.history = []\n",
    "        self.history_timestamps = []\n",
    "        self.speed = 0\n",
    "        self.is_static = False\n",
    "        self.is_active=True\n",
    "        self.is_violating = False\n",
    "        \n",
    "        \n",
    "    def is_violating_sd(self, ped2, threshold): # 182.88 cm = 6 ft. threshold = 100 cm according to WHO  \n",
    "        '''\n",
    "        return true if self and ped2 are too close\n",
    "        '''\n",
    "        distance = np.linalg.norm(ped2.center - self.center) - ped2.radius - self.radius\n",
    "#         distance *= worlddist2cm \n",
    "        if distance < threshold: \n",
    "            return True\n",
    "        else: \n",
    "            return False \n",
    "        \n",
    "##### TODO: not used / tested yet ####\n",
    "class PedestrianManager: \n",
    "    def __init__(self, FPS, history_size=5, speed_tolerance = 10, threshold=5): \n",
    "        self.FPS = FPS\n",
    "        self.history_size = history_size\n",
    "        self.speed_tolerance = speed_tolerance\n",
    "        self.threshold = threshold\n",
    "        self.pedestrians_dict = dict() # stores all pedestrians including those that are no longer active \n",
    "        self.current_time = 0\n",
    "        self.active_ids = set() # set of all pedestrian IDs that are still active in the video \n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.pedestrians_dict)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.pedestrians_dict)\n",
    "    \n",
    "    def __contains__(self, pedestrian_id): \n",
    "        if pedestrian_id in self.pedestrians_dict.keys(): \n",
    "            return True\n",
    "        else: \n",
    "            return False\n",
    "        \n",
    "    def __getitem__(self, pedestrian_id): \n",
    "        return self.pedestrians_dict[pedestrian_id]\n",
    "    \n",
    "    def __setitem__(self, pedestrian_id, p): \n",
    "        history = []\n",
    "        history_timestamps = []\n",
    "        if pedestrian_id in self: \n",
    "            # not a new pedestrian, get old history\n",
    "            history = self[pedestrian_id].history\n",
    "            history_timestamps = self[pedestrian_id].history_timestamps\n",
    "            \n",
    "        # initialize a new active pedestrian with updated history \n",
    "        history.append(p.center)\n",
    "        history_timestamps.append(self.current_time)\n",
    "        \n",
    "        # remove oldest history\n",
    "        if len(history) > self.history_size: \n",
    "            history.pop(0)\n",
    "            history_timestamps.pop(0)\n",
    "            \n",
    "        p.history = history\n",
    "        p.history_timestamps = history_timestamps\n",
    "        \n",
    "        # calculate current speed of the pedestrian \n",
    "        if len(history) > 1:\n",
    "            p.speed = np.linalg.norm(history[-1] - history[-2]) / (p.history_timestamps[-1] - p.history_timestamps[-2]) # distance / time\n",
    "        else: \n",
    "            p.speed = 0\n",
    "            \n",
    "        if p.speed <= self.speed_tolerance: \n",
    "            p.is_static = True\n",
    "            \n",
    "        # store it in our container class  \n",
    "        self.active_ids.add(pedestrian_id)\n",
    "        self.pedestrians_dict[pedestrian_id] = p\n",
    "        \n",
    "    def __set_inactive(self, pedestrian_id):\n",
    "        if pedestrian_id in self.active_ids:\n",
    "            self.active_ids\n",
    "        else:\n",
    "            raise Exception(\"Pedestrian {} is not currently active\".format(pedestrian_id))\n",
    "            \n",
    "    def advance_time(self): \n",
    "        # set as inactive if no data in last frame \n",
    "        new_set = set()\n",
    "        for k in self.active_ids: \n",
    "            if self[k].history_timestamps[-1] != self.current_time:\n",
    "                # inactive\n",
    "                self[k].is_active = False\n",
    "            else:\n",
    "                # active \n",
    "                new_set.add(k)\n",
    "        self.active_ids = new_set\n",
    "        # advance time by 1 frame\n",
    "        self.current_time += 1 / self.FPS \n",
    "        \n",
    "    def calc_num_violated(self):\n",
    "        from itertools import combinations \n",
    "        \n",
    "        violating = set()\n",
    "        for (id1,id2) in combinations(self.active_ids, 2): \n",
    "            p1 = self[id1]\n",
    "            p2 = self[id2]\n",
    "            if (not p1.is_static) and (not p2.is_static) and p1.is_violating_sd(p2, self.threshold):\n",
    "                assert p2.is_violating_sd(p1, self.threshold)\n",
    "                p1.is_violating=True\n",
    "                p2.is_violating=True\n",
    "                violating.add(p1.id)\n",
    "                violating.add(p2.id)\n",
    "        \n",
    "        num_violated = len(violating)\n",
    "        return num_violated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "# initialize model and detectron2 visualizer \n",
    "if detector == 'faster_rcnn':\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "elif detector == 'yolov3': \n",
    "    model = Darknet(yolov3_config_path, img_size=scale_size) #torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #\n",
    "    model.load_weights(yolov3_weight_path)\n",
    "elif detector == 'detectron2': \n",
    "    model = DefaultPredictor(cfg)\n",
    "    detectron2_visualizer = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), ColorMode.IMAGE)\n",
    "else: \n",
    "    raise Exception('Invalid detector name')\n",
    "    \n",
    "# initialize object tracker\n",
    "mot_tracker = Sort() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972143b1b360401ea6c55bd58a4e057a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load video\n",
    "cap = cv2.VideoCapture(os.path.join('data', video_filename))\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "result1 = cv2.VideoWriter(result_path_detection, \n",
    "                cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                FPS, \n",
    "                (width, height))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "camera = Camera(fig)\n",
    "# container class \n",
    "all_pedestrians = PedestrianManager(FPS, history_size=history_size, speed_tolerance=speed_tolerance)\n",
    "\n",
    "# while cap.isOpened():\n",
    "for ii in tqdm(range(700)): # just do this many frames for now \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret is False:\n",
    "        break\n",
    "        \n",
    "    prediction = detect_image(frame, model, detector_name = detector, scale_size = scale_size)\n",
    "    \n",
    "    if prediction is None: \n",
    "        break\n",
    "        \n",
    "    # plot boxes on frame and show inline\n",
    "    pilimg = Image.fromarray(frame)\n",
    "    img = np.array(pilimg)\n",
    "    if scale_size:\n",
    "        # need to resize box to account for how image was padded to square\n",
    "        pad_x = max(img.shape[0] - img.shape[1], 0) * (scale_size / max(img.shape))\n",
    "        pad_y = max(img.shape[1] - img.shape[0], 0) * (scale_size / max(img.shape))\n",
    "        unpad_h = scale_size - pad_y\n",
    "        unpad_w = scale_size - pad_x\n",
    "    else: \n",
    "        # no padding.\n",
    "        pad_x = 0 \n",
    "        pad_y = 0\n",
    "        unpad_h = img.shape[0]\n",
    "        unpad_w = img.shape[1]\n",
    "\n",
    "    # draw segmentation mask if detectron\n",
    "    if detector == 'detectron2': \n",
    "        if \"panoptic_seg\" in prediction[\"detectron2\"].keys():\n",
    "            panoptic_seg, segments_info = prediction[\"detectron2\"][\"panoptic_seg\"]\n",
    "            out = detectron2_visualizer.draw_panoptic_seg_predictions(frame, panoptic_seg.to(\"cpu\"), segments_info)\n",
    "        else: \n",
    "            out = detectron2_visualizer.draw_instance_predictions(frame, prediction[\"detectron2\"][\"instances\"].to(\"cpu\"))\n",
    "        frame = out.get_image() # BGR\n",
    "\n",
    "    # track and visualize for regular detection boxes and ids. \n",
    "    tracked_objects = mot_tracker.update(prediction[\"darknet\"].cpu())\n",
    "    unique_labels = prediction[\"darknet\"][:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
    "        obj_id = int(obj_id)\n",
    "        box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
    "        box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
    "        y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
    "        x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
    "\n",
    "        color = colors[obj_id % len(colors)]\n",
    "        color = [i * 255 for i in color]\n",
    "        cls = CLASSES[int(cls_pred)]\n",
    "        if cls == 'person' : # 0: person\n",
    "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 1)\n",
    "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n",
    "            all_pedestrians[obj_id] = Pedestrian((x1, y1), (x1+box_w, y1+box_h), int(obj_id), cls, box_h, box_w)\n",
    "    \n",
    "    # see if anyone's too close? \n",
    "    num_proximity = all_pedestrians.calc_num_violated()\n",
    "    cv2.putText(frame, \"Number of People: \"+str(len(all_pedestrians.active_ids))+\"; Proximity alert: \"+str(num_proximity), (10,frame.shape[0]-5), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 3)\n",
    "    \n",
    "    # show 2D transformed world coordinates\n",
    "    if ii == 0: \n",
    "        # initiate points so that we get a good axis proportion\n",
    "        ax.scatter(*np.vstack([all_pedestrians[i].center for i in all_pedestrians.active_ids]).T, color='k', s=0.0001)\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "    for pedestrian_id in all_pedestrians.active_ids: \n",
    "        p = all_pedestrians[pedestrian_id]\n",
    "        #  red color if violated \n",
    "        if p.is_violating: \n",
    "            color=(1.0,0.0,0.0)\n",
    "            ax.annotate('colliding', p.center)\n",
    "        else: \n",
    "            color = colors[int(p.id) % len(colors)] \n",
    "            # convert from cv2 BGR color to RGB\n",
    "            color = (color[2], color[1], color[0])\n",
    "        if p.is_static: \n",
    "            ax.annotate('static', p.center)\n",
    "#         ax.annotate(p.id, p.center)\n",
    "        circle = plt.Circle(p.center, p.radius, color=color)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "#     break\n",
    "    #######save video to results folder###############\n",
    "    result1.write(frame)\n",
    "    camera.snap()\n",
    "    #################################################\n",
    "    \n",
    "    # always call this before looking at the next frame \n",
    "    all_pedestrians.advance_time()\n",
    "\n",
    "anim = camera.animate(blit=True) # save matplotlib animation with celloid\n",
    "anim.save(result_path_world, fps=FPS)\n",
    "\n",
    "cap.release()\n",
    "result1.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = camera.animate(blit=True) # save matplotlib animation with celloid\n",
    "anim.save(result_path_world, fps=FPS)\n",
    "\n",
    "cap.release()\n",
    "result1.release()\n",
    "# result2.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (for debug only)\n",
    "# check homography matrix \n",
    "%matplotlib inline\n",
    "pts_img = [[1090, 1625], [2270, 1785], [2010, 2250], [470, 2150]] # palm tree locations \n",
    "print(pts_img)\n",
    "print(pts_world)\n",
    "\n",
    "flong_lat_world = [(37.428570, -122.175184), (37.428489, -122.175211), \n",
    "                  (37.428506, -122.175288), (37.428589, -122.175260)]\n",
    "pts_world = []\n",
    "for lat,lon in long_lat_world: \n",
    "    utm_east, utm_north, utm_zone, utm_letter = utm.from_latlon(lat,lon) # meters\n",
    "    pts_world.append([utm_east*100, utm_north*100]) # centimeters\n",
    "# shift pts_world so that values aren't too big\n",
    "world_np = np.array(pts_world)\n",
    "world_np = world_np - world_np.min(axis=0)\n",
    "pts_world = world_np.tolist()\n",
    "\n",
    "\n",
    "HOMOG, status = cv2.findHomography(np.array(pts_img), np.array(pts_world))\n",
    "\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(18,18))\n",
    "pts_test_img = [[100, 100], [200, 300], [1000, 1000], [2000, 3000]]\n",
    "pts_test_world = img_to_world_coords(np.array([pts_test_img]), HOMOG)[0,:,:].T\n",
    "print(pts_test_world.shape)\n",
    "ax2.scatter(*pts_test_world)\n",
    "ax2.set_aspect('equal')\n",
    "# ax1.imshow(cv2.cvtColor(warped_frame, cv2.COLOR_BGR2RGB))\n",
    "# ax2.imshow(warped_frame)\n",
    "ax3.imshow(frame)\n",
    "ax3.scatter(*np.array(pts_test_img).T)\n",
    "for i, (ptx, pty) in enumerate(pts_img): \n",
    "    ax3.scatter(ptx, pty)\n",
    "    ax3.annotate(i, (ptx, pty), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video (old, no container / manager class) \n",
    "cap = cv2.VideoCapture(os.path.join('data', video_filename))\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "result1 = cv2.VideoWriter(result_path_detection, \n",
    "                cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                FPS, \n",
    "                (width, height))\n",
    "# result2 = cv2.VideoWriter(result_path_world, \n",
    "#                 cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "#                 FPS, \n",
    "#                 (1296,1296)) # matplotlib figsize (18,18) -> 1296\n",
    "fig, ax = plt.subplots(figsize=(18,18))\n",
    "camera = Camera(fig)\n",
    "\n",
    "# while cap.isOpened():\n",
    "for ii in tqdm(range(1000)): # just do this many frames for now \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret is False:\n",
    "        break\n",
    "        \n",
    "    prediction = detect_image(frame, model, detector_name = detector, scale_size = scale_size)\n",
    "    \n",
    "    if prediction is None: \n",
    "        break\n",
    "        \n",
    "    # plot boxes on frame and show inline\n",
    "    pilimg = Image.fromarray(frame)\n",
    "    img = np.array(pilimg)\n",
    "    if scale_size:\n",
    "        # need to resize box to account for how image was padded to square\n",
    "        pad_x = max(img.shape[0] - img.shape[1], 0) * (scale_size / max(img.shape))\n",
    "        pad_y = max(img.shape[1] - img.shape[0], 0) * (scale_size / max(img.shape))\n",
    "        unpad_h = scale_size - pad_y\n",
    "        unpad_w = scale_size - pad_x\n",
    "    else: \n",
    "        # no padding.\n",
    "        pad_x = 0 \n",
    "        pad_y = 0\n",
    "        unpad_h = img.shape[0]\n",
    "        unpad_w = img.shape[1]\n",
    "\n",
    "    # draw segmentation mask if detectron\n",
    "    if detector == 'detectron2': \n",
    "        if \"panoptic_seg\" in prediction[\"detectron2\"].keys():\n",
    "            panoptic_seg, segments_info = prediction[\"detectron2\"][\"panoptic_seg\"]\n",
    "            out = detectron2_visualizer.draw_panoptic_seg_predictions(frame, panoptic_seg.to(\"cpu\"), segments_info)\n",
    "        else: \n",
    "            out = detectron2_visualizer.draw_instance_predictions(frame, prediction[\"detectron2\"][\"instances\"].to(\"cpu\"))\n",
    "        frame = out.get_image() # BGR\n",
    "\n",
    "    # track and visualize for regular detection boxes and ids. \n",
    "    tracked_objects = mot_tracker.update(prediction[\"darknet\"].cpu())\n",
    "    \n",
    "    all_pedestrians = []\n",
    "    unique_labels = prediction[\"darknet\"][:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
    "        box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
    "        box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
    "        y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
    "        x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
    "\n",
    "        color = colors[int(obj_id) % len(colors)]\n",
    "        color = [i * 255 for i in color]\n",
    "        cls = CLASSES[int(cls_pred)]\n",
    "        if cls == 'person' : # 0: person\n",
    "            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 1)\n",
    "            cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n",
    "            all_pedestrians.append(Pedestrian((x1, y1), (x1+box_w, y1+box_h), int(obj_id), cls, box_h, box_w))\n",
    "    \n",
    "    # see if anyone's too close? \n",
    "    num_proximity = calc_num_violated (all_pedestrians)\n",
    "    cv2.putText(frame, \n",
    "                \"Number of People: \"+str(len(all_pedestrians))+\"; Proximity alert: \"+str(num_proximity), \n",
    "                (10,frame.shape[0]-5), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                2, \n",
    "                (0,0,255), \n",
    "                3)\n",
    "    \n",
    "    # show 2D transformed world coordinates\n",
    "    if ii == 0: \n",
    "        # initiate points so that we get a good axis proportion\n",
    "        ax.scatter(*np.vstack([p.center for p in all_pedestrians]).T, color='k', s=0.0001)\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "    for p in all_pedestrians: \n",
    "        #  red color if violated \n",
    "        if p.is_violating: \n",
    "            p.plot_color=(1.0,0.0,0.0)\n",
    "        else: \n",
    "            p.plot_color = colors[int(p.id) % len(colors)]\n",
    "\n",
    "    #         ax.annotate(p.id, p.center)\n",
    "        circle = plt.Circle(p.center, p.radius, color=p.plot_color)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "#     break\n",
    "    #######save video to results folder###############\n",
    "    result1.write(frame)\n",
    "    camera.snap()\n",
    "#     result2.write(warped_frame)\n",
    "    #################################################\n",
    "\n",
    "anim = camera.animate(blit=True) # save matplotlib animation with celloid\n",
    "anim.save(result_path_world, fps=FPS)\n",
    "\n",
    "cap.release()\n",
    "result1.release()\n",
    "# result2.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "vid = cv2.VideoCapture(videopath)\n",
    "width  = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# initialize video writer \n",
    "result = cv2.VideoWriter(resultpath_detectron2, \n",
    "                cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                fps, \n",
    "                (width, height))\n",
    "\n",
    "# initialize video visualizer\n",
    "v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), ColorMode.IMAGE)\n",
    "\n",
    "# while(True):\n",
    "for ii in tqdm(range(600)): # just do this many number of frames for now \n",
    "    ret, frame = vid.read()\n",
    "    outputs = predictor(frame)\n",
    "    out = v.draw_instance_predictions(frame, outputs[\"instances\"].to(\"cpu\"))\n",
    "    im = out.get_image()\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Write to video file\n",
    "    result.write(im)\n",
    "\n",
    "#     # (for debugging) Show output frame\n",
    "#     plt.imshow(im)\n",
    "#     break\n",
    "\n",
    "vid.release()\n",
    "result.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panoptic segmentation w/ detectron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test with screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread('data/test_screenshot.png')\n",
    "plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE = 'cpu' # Added for CPU only inference\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
    "print(outputs[\"instances\"].pred_classes)\n",
    "print(outputs[\"instances\"].pred_boxes)\n",
    "\n",
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "im = out.get_image()[:, :, ::-1]\n",
    "plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(50,50))\n",
    "plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now test it with video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "vid = cv2.VideoCapture('data/Y2E2_West.MOV')\n",
    "width  = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "num_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# initialize video writer \n",
    "result = cv2.VideoWriter('results/Y2E2_West.mp4', \n",
    "                cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                fps, \n",
    "                (width, height))\n",
    "\n",
    "# initialize video visualizer\n",
    "v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), ColorMode.IMAGE)\n",
    "\n",
    "# while(True):\n",
    "for ii in tqdm(range(600)): # just do this many number of frames for now \n",
    "    ret, frame = vid.read()\n",
    "    outputs = predictor(frame)\n",
    "    out = v.draw_instance_predictions(frame, outputs[\"instances\"].to(\"cpu\"))\n",
    "    im = out.get_image()\n",
    "\n",
    "    # Write to video file\n",
    "    result.write(im)\n",
    "\n",
    "#     # (for debugging) Show output frame\n",
    "#     plt.imshow(im)\n",
    "#     break\n",
    "\n",
    "vid.release()\n",
    "result.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv3 detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='config/yolov3.cfg'\n",
    "weights_path='weights/yolov3.weights'\n",
    "class_path='config/coco.names'\n",
    "img_size=416\n",
    "conf_thres=0.8\n",
    "nms_thres=0.4\n",
    "\n",
    "# Load model and weights\n",
    "model = Darknet(config_path, img_size=img_size)\n",
    "model.load_weights(weights_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "classes = utils.load_classes(class_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video specific magic numbers\n",
    "assumed_height = 174\n",
    "pts_img = [[1090, 1575], [2270, 1725], [470, 2050],[2010, 2150] ] # palm tree locations \n",
    "long_lat_world = [(37.428570, -122.175184), (37.428489, -122.175211), \n",
    "                  (37.428589, -122.175260), (37.428506, -122.175288)]\n",
    "pts_world = []\n",
    "for lat,lon in long_lat_world: \n",
    "    utm_east, utm_north, utm_zone, utm_letter = utm.from_latlon(lat,lon) # meters\n",
    "    pts_world.append([utm_east*100, utm_north*100]) # centimeters\n",
    "# shift pts_world so that values aren't too big\n",
    "world_np = np.array(pts_world)\n",
    "world_np = world_np - world_np.min(axis=0)\n",
    "pts_world = world_np.tolist()\n",
    "\n",
    "    \n",
    "h, status = cv2.findHomography(np.array(pts_img), np.array(pts_world))\n",
    "\n",
    "#### converting any coordinate to world coordinate \n",
    "def img_to_world_coords(coords_img, pts_img, pts_world):\n",
    "    '''\n",
    "    use at least 4 \n",
    "    input: \n",
    "        coords_img: a np array of dim (num_ped,seq_len,2)\n",
    "        pts_img: a list of [x,y] points in image coordinates. list length cannot be shorter than 4\n",
    "        pts_world: a list of [x,y] points in world coordinates. each point cooresponds to a point in pts_img\n",
    "        assumed_height: the estimated height of all humans. source: https://www.sciencedirect.com/science/article/pii/S0379073811002167\n",
    "                        NOT USED FOR NOW \n",
    "    output: \n",
    "        coords_world: same shape as coords_img but in world coordinates\n",
    "    '''\n",
    "    # convert to np matrix and check size\n",
    "\n",
    "    pts_img = np.array(pts_img)\n",
    "    pts_world = np.array(pts_world)\n",
    "    assert pts_img.shape==pts_world.shape and pts_img.shape[0] >= 4\n",
    "     \n",
    "    # calculate homography matrix H. more pts_img means more accurate H\n",
    "    h, status = cv2.findHomography(pts_img, pts_world)\n",
    "     \n",
    "    # finally, get the mapped world coordinates\n",
    "    coords_world = cv2.perspectiveTransform(coords_img.astype(np.float32), h)\n",
    "    # coords_world[:,1] += assumed_height\n",
    "    return coords_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pylab inline \n",
    "# from IPython.display import clear_output\n",
    "\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "# for pd dataframe\n",
    "raw_data_list = []\n",
    "csv_columns = [\"frame_id\", \"agent_id\", \"pos_x\", \"pos_y\"]\n",
    "\n",
    "# initialize Sort object\n",
    "mot_tracker = Sort() \n",
    "\n",
    "result1 = cv2.VideoWriter(resultpath_detection, \n",
    "                cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                fps, \n",
    "                (width, height))\n",
    "result2 = cv2.VideoWriter(resultpath_world, \n",
    "                cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                fps, \n",
    "                (width, height))\n",
    "# while(True):\n",
    "for ii in tqdm(range(600)): # just do this many number of frames for now \n",
    "    ret, frame = vid.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pilimg = Image.fromarray(frame)\n",
    "    detections = detect_image(pilimg)\n",
    "\n",
    "    img = np.array(pilimg)\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "    unpad_h = img_size - pad_y\n",
    "    unpad_w = img_size - pad_x\n",
    "    ########### vivian initialize ############\n",
    "    all_pedestrians = [] # all of the people in this frame \n",
    "    ##########################################\n",
    "    if detections is not None:\n",
    "        tracked_objects = mot_tracker.update(detections.cpu())\n",
    "\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
    "            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
    "            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
    "            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
    "            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
    "\n",
    "            cls = classes[int(cls_pred)]\n",
    "            if cls == 'person' or cls == 'bicycle': # 0: person, 1: bicycle\n",
    "                color = colors[int(obj_id) % len(colors)]\n",
    "                color = [i * 255 for i in color]\n",
    "                cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
    "                cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60, y1), color, -1)\n",
    "                cv2.putText(frame, cls + \"-\" + str(obj_id), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
    "                # vivian added\n",
    "                all_pedestrians.append(Pedestrian((x1, y1), (x1+box_w, y1+box_h), obj_id, cls, box_h, box_w))\n",
    "    \n",
    "    #### plotting and showing social distancing violations in video frames ####\n",
    "    # social distancing \n",
    "#     num_violated = calc_num_violated (all_pedestrians)\n",
    "#     cv2.putText(frame, \"Number of People: \"+str(len(all_pedestrians))+\"; Violations: \"+str(num_violated), (10,frame.shape[0]-5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 3)\n",
    "    \n",
    "    # show 2D transformed world coordinates\n",
    "    warped_frame = cv2.warpPerspective(frame, h, dsize=(width,height))\n",
    "    warped_frame *= 0 # make all black \n",
    "    for p in all_pedestrians:\n",
    "        color = colors[int(p.id) % len(colors)]\n",
    "        color = [i * 255 for i in color]\n",
    "        draw_circle(warped_frame, p.center, p.radius, color)\n",
    "        # overlap red color if violated \n",
    "#         if p.is_violating: \n",
    "#             color = (255,0,0)\n",
    "#             cv2.rectangle(frame, (p.x1, p.y1), (p.x2, p.y2), color, 4)\n",
    "#             draw_circle(warped_frame, p.center, p.radius, color)\n",
    "        # append to raw data to store in data frame \n",
    "        raw_data_list.append([ii, p.id, p.center[0], p.center[1]])  \n",
    "        \n",
    "    ####### show videos in jupyter notebook inline #######\n",
    "#     fig1=figure(figsize=(12, 8))\n",
    "#     title(\"Video Stream\")\n",
    "#     imshow(frame)\n",
    "    \n",
    "#     fig2=figure(figsize=(12,8))\n",
    "#     title(\"World Coordinate\")\n",
    "#     imshow(warped_frame)\n",
    "    #######save video to results folder###############\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    result1.write(frame)\n",
    "    warped_frame = cv2.cvtColor(warped_frame, cv2.COLOR_RGB2BGR)\n",
    "    result2.write(warped_frame)\n",
    "    #################################################\n",
    "#     show()\n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid.release()\n",
    "result1.release()\n",
    "result2.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eyeballing for homography matrix\n",
    "4 palm trees on google map are: (37.428570, -122.175184), (37.428489, -122.175211), \n",
    "                                (37.428589, -122.175260), (37.428506, -122.175288)    \n",
    "on graph they are approximately (1090, 1575), (2270, 1725), (470, 2050),(2010, 2150)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(videopath)\n",
    "ret, frame = vid.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "figure, axes = plt.subplots(1,2 )\n",
    "axes[0].imshow(frame)\n",
    "axes[0].grid(which='both')\n",
    "\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "axes[0].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "axes[0].yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "image_pts = [(1090, 1575), (2270, 1725), (470, 2050),(2010, 2150) ]\n",
    "for x,y in image_pts: \n",
    "    axes[0].scatter(x,y)\n",
    "\n",
    "# world coord\n",
    "world_pts = [(37.428570, -122.175184), (37.428489, -122.175211), \n",
    "             (37.428589, -122.175260), (37.428506, -122.175288)]\n",
    "for lat,lon in world_pts: \n",
    "    utm_east, utm_north, utm_zone, utm_letter = utm.from_latlon(lat,lon) # meters\n",
    "    axes[1].scatter(utm_east, utm_north) \n",
    "    \n",
    "axes[1].axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.warpPerspective(frame, h, dsize=(1500,1500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pts_world)-np.array(pts_world).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.warpPerspective(frame, h, dsize=(1500,1500)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
